{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "pyds_02_introduction_to_pandas.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cristripoli/Codility/blob/master/pyds_02_introduction_to_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8c8a08"
      },
      "source": [
        "# <img src=\"https://raw.githubusercontent.com/daitan-innovation/daitan-ml-course-resources/main/daitan-header.jpg\" width=\"700\">\n",
        "\n",
        "<br />\n",
        "\n",
        "<span style=\"font-size: 10px; font-style: italic;\">\n",
        "Privileged and confidential. If this content has been received in error, please delete it immediately.\n",
        "<br />\n",
        "Conteúdo confidencial. Se este material foi recebido por engano, por favor apague-o imediatamente.\n",
        "</span>"
      ],
      "id": "eb8c8a08"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb93c33"
      },
      "source": [
        "# Trilha de Machine Learning\n",
        "\n",
        "- **Module.** Python for Data Science\n",
        "- **Instructors:**\n",
        "  - Alisson Hayasi da Costa\n",
        "  - Lucas Silveira de Moura"
      ],
      "id": "4eb93c33"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c492e4"
      },
      "source": [
        "# *pandas* Basics\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this lesson you will learn the basics of the *pandas* library, including:\n",
        "- Pandas Series and its most useful operations\n",
        "- Pandas DataFrames and its most useful operations\n",
        "\n",
        "Here's a bit about what pandas is all about:\n",
        "*pandas* is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python.\n",
        "\n",
        "*pandas* is well suited for many different kinds of data:\n",
        "- Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
        "- Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
        "- Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n",
        "- Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure\n",
        "\n",
        "The two primary data structures of *pandas*, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. *pandas* is built on top of NumPy (from the previous class) and is intended to integrate well within a scientific computing environment with many other 3rd party libraries (as, for example, jupyter itself)."
      ],
      "id": "f5c492e4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4960f8be"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "4960f8be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c960f1b6"
      },
      "source": [
        "### *pandas* [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)\n",
        "Pandas Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, objects, etc.). The axis labels are collectively called index. Labels need not be unique but must be a hashable type. The object provides a host of methods for performing operations involving both the index and the values.\n",
        "\n",
        "Let's take a look at the most common way in which we can create a Series:"
      ],
      "id": "c960f1b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62f3ab5"
      },
      "source": [
        "#### Series creation\n",
        "Creating a Series by passing a list of values, letting pandas create a default integer index:\n",
        "<br/>OBS: This also works by passing a NumPy array instead of a common Python list."
      ],
      "id": "d62f3ab5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3372ac5e"
      },
      "source": [
        "s = pd.Series([1, 3, 5, np.nan, 6, 8])\n",
        "s"
      ],
      "id": "3372ac5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a7d6a20"
      },
      "source": [
        "Using the default visualization method of Series in a Jupyter notebook like this one we can observe:\n",
        "- On the left column, we can see the index. In this case, it was automatically generated keeping the order of the inserted list, and starting at 0 with increments of 1 (from 0 to 5 as we have 6 elements).\n",
        "- On the right column, we can see the values themselves.\n",
        "- At the bottom, we have the type of the values in this Series (in this case, float64).\n",
        "\n",
        "#### Accessing an element of a Series\n",
        "Let's access a value in the Series by informing its index:"
      ],
      "id": "8a7d6a20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0729e743"
      },
      "source": [
        "s[2]"
      ],
      "id": "0729e743",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3378a279"
      },
      "source": [
        "#### Index of Series\n",
        "If desired, a manually-defined index can be specified at the creation of the Series:"
      ],
      "id": "3378a279"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2e79453"
      },
      "source": [
        "s = pd.Series([1, 3, 5, np.nan, 6, 8], index=['a', 'b', 'c', 'd', 'e', 'f'])\n",
        "s"
      ],
      "id": "e2e79453",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6abe9b6"
      },
      "source": [
        "And now let's access the same value as before, but now with a different index:"
      ],
      "id": "a6abe9b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e22714e"
      },
      "source": [
        "s['c']"
      ],
      "id": "4e22714e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4685325e"
      },
      "source": [
        "Another commonly used way to access a Series element based on its index is to use the *loc[index]* notation:"
      ],
      "id": "4685325e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e397c570"
      },
      "source": [
        "s.loc['c']"
      ],
      "id": "e397c570",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a10628e"
      },
      "source": [
        "#### Naming a Series\n",
        "A Series can be [named](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.name.html). Naming a Series can be useful when using it to compose a DataFrame (will be covered later in this class)."
      ],
      "id": "2a10628e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2665ce4"
      },
      "source": [
        "s.name = 'Example Series'\n",
        "s"
      ],
      "id": "a2665ce4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74b464e"
      },
      "source": [
        "#### Deleting an element from a Series\n",
        "Elements from the Series can be deleted (or *dropped*) by using the [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.drop.html) method and passing the list of indexes to be removed.\n",
        "Notice that the size of the Series will be affected when at least one element is dropped, instead of marking the elements as empty and keeping the size."
      ],
      "id": "c74b464e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1777ef8"
      },
      "source": [
        "s = s.drop(['a', 'e'])\n",
        "s"
      ],
      "id": "f1777ef8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f959778"
      },
      "source": [
        "#### Sorting a Series\n",
        "We can also sort Series by either its [values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sort_values.html) or by its [index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sort_index.html)."
      ],
      "id": "3f959778"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "904fc576"
      },
      "source": [
        "s = s.sort_index(ascending=False)\n",
        "s"
      ],
      "id": "904fc576",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1a188e5"
      },
      "source": [
        "s = s.sort_values(ascending=False)\n",
        "s"
      ],
      "id": "c1a188e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc2ffcfb"
      },
      "source": [
        "If you want to access an element in the Series by its position, respecting only the current order the Series is ordered by, use the *iloc[position]* notation (0-based):"
      ],
      "id": "dc2ffcfb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c369267e"
      },
      "source": [
        "s.iloc[2]"
      ],
      "id": "c369267e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3a9e805"
      },
      "source": [
        "#### Useful Series methods\n",
        "Another useful operation for Series is [rank()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rank.html). It ranks the values when possible, keeping the order of the Series. In the example below we can see that the non-numeric value (NaN) does not get ranked, and its rank instead shows as NaN as well. By default, tied values will have the same rank, but will keep the general count of elements in the rank (i.e., in a Series with 10 rankable elements where a single element is the highest, its rank will be also 10)."
      ],
      "id": "e3a9e805"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60a2b4ec"
      },
      "source": [
        "ranked_s = pd.Series([1, 2, 3, 3, 3, np.nan, 4, 5])\n",
        "ranked_s"
      ],
      "id": "60a2b4ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6441aad"
      },
      "source": [
        "ranked_s.rank()"
      ],
      "id": "c6441aad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "632c6651"
      },
      "source": [
        "A quick and easy way to know how many non-NaN/null elements are present in a series is the [count()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.count.html) method. Let's compare it to simply computing the length of a Series:"
      ],
      "id": "632c6651"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0271def8"
      },
      "source": [
        "print(f'The length of the Series \"ranked_s\" is {len(ranked_s)} but its count of non-null elements is {ranked_s.count()}')"
      ],
      "id": "0271def8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7adfc1f"
      },
      "source": [
        "We can also check how many elements per value are present in a series by using the [`value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html) method. The result is a new Series where the indexes are the actual values found in the original Series, and the values are the counts of the respective value indicated by the index present. If you want to account for the null values as well, use the `dropna=False` parameter."
      ],
      "id": "c7adfc1f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b825a4cf"
      },
      "source": [
        "ranked_s.value_counts(dropna=False)"
      ],
      "id": "b825a4cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed0d960"
      },
      "source": [
        "If you wish to fill in the values that are Null/NaN in a Series, use the [`fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) method, passing as a parameter the value that the nulls should be replaced by:"
      ],
      "id": "0ed0d960"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ebff41"
      },
      "source": [
        "ranked_s.fillna(10000)"
      ],
      "id": "96ebff41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54f7135a"
      },
      "source": [
        "Notice that this will return a new Series with the replaced values. If you want to apply this change to the original Series, use the `inplace=True` parameter as well."
      ],
      "id": "54f7135a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a35c29"
      },
      "source": [
        "Let's now take a look at some mathematical operations we can do with a single Series.\n",
        "<br/><br/>A quick way to apply any operation to all elements in a Series is to use the [apply()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html) method. Its result is going to be a new Series where each value is the result of the defined operation applied to the respective element of the original Series. You can define any method this way. As an example, let's take the square of each element in our Series."
      ],
      "id": "62a35c29"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0608b7ff"
      },
      "source": [
        "def square(value):\n",
        "    return value*value\n",
        "\n",
        "z = s.apply(square)\n",
        "z"
      ],
      "id": "0608b7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d866130d"
      },
      "source": [
        "If the method is simple enough, like the one shown above, you can also use lambda notation to make it even smaller and simpler:"
      ],
      "id": "d866130d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "529bb585"
      },
      "source": [
        "z = s.apply(lambda value: value*value)\n",
        "z"
      ],
      "id": "529bb585",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "352bf0cd"
      },
      "source": [
        "In order to find the sum of all values in a Series, use the method [sum()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sum.html). It is automatically going to ignore the null/non-numeric values, unless otherwise specified."
      ],
      "id": "352bf0cd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3807a30"
      },
      "source": [
        "z = s.sum()\n",
        "z"
      ],
      "id": "c3807a30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2730778"
      },
      "source": [
        "You can use similar methods to the sum() described above in order to extract other kinds of values from the Series, like:\n",
        "- [mean()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mean.html): Mean value (will work for numeric types only)\n",
        "- [std()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.std.html): Standard deviation (will work for numeric types only)\n",
        "- [min()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.min.html): Minimum value\n",
        "- [max()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.max.html): Maximum value\n",
        "- [median()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.median.html): Median value (will work for numeric types only)\n",
        "- [mode()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mode.html): Most frequent value or mode. If there values tied for most frequent, multiple will be returned in a new Series.\n",
        "\n",
        "In order for any of these methods to consider null values, you should pass `skipna=False` as a parameter."
      ],
      "id": "a2730778"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f88ce87"
      },
      "source": [
        "#### Operations between 2 Series\n",
        "Common mathematical operations between 2 Series are also possible. Notice however, that these operations will use the indexes of both Series in order to match the elements. Let's create another Series to test some of these operations with our old Series *s*, and also remember what *s* is so it all makes a little more sense."
      ],
      "id": "8f88ce87"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aff7b157"
      },
      "source": [
        "s"
      ],
      "id": "aff7b157",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "886b7c54"
      },
      "source": [
        "s2 = pd.Series([np.nan, 2, np.nan, 4], index=['a', 'b', 'c', 'd'], name=\"Example Series 2\")\n",
        "s2"
      ],
      "id": "886b7c54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec73679a"
      },
      "source": [
        "Let's begin with the [add()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.add.html) operation, which will add the elements of the specified Series to the elements of the caller Series. In the example we're going to use the `fill_value` parameter in order to specify what should be the value that overwrites the null/non-numeric values as well as unmatched values from either Series."
      ],
      "id": "ec73679a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d515d73c"
      },
      "source": [
        "s.add(s2, fill_value=0)"
      ],
      "id": "d515d73c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "778ecf37"
      },
      "source": [
        "Let's take a moment to understand the results above:\n",
        "- *a* gets resolved to \"NaN\" since *a* was present only in the \"s2\" Series, and there its value was NaN.\n",
        "- *b* gets resolved to 5.0 since in the \"s\" Series its value was 3.0 and in the \"s2\" Series its value was 2.0 (2.0 + 3.0 = 5.0)\n",
        "- *c* gets resolved to 5.0 since in the \"s\" Series its value was 5.0 and in the \"s2\" Series its value was NaN, which due to \"fill_value=0\" got replaced by 0 (5.0 + 0 = 5.0).\n",
        "- *d* gets resolved to 4.0 since in the \"s\" Series its value was NaN which due to \"fill_value=0\" gets replaced by 0, and in the \"s2\" Series its value was 4.0 (0 + 4.0 = 4.0).\n",
        "- *f* gets resolved to 8.0 since it only exists in the \"s\" Series, where its value was 8.0. It is then added to the \"fill_value\" of 0 (8.0 + 0 = 8.0).\n",
        "\n",
        "In a very similar fashion to the add() method described above, Series can also be subtracted, multiplied and divided by each other by using [sub()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.sub.html) [mul()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.mul.html) and [div()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.div.html) respectively."
      ],
      "id": "778ecf37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f1017e"
      },
      "source": [
        "### *pandas* [DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html)\n",
        "Pandas DataFrame is two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). Pandas DataFrames consist of three principal components, rows, columns and the data.\n",
        "\n",
        "With a basic understanging of *pandas* Series, it becomes much easier to grasp the concept of DataFrames, which can be perceived as a Series of Series. For example, you can interpret each a row in a *pandas* DataFrame as a Series, where its index is going to be the column labels of that DataFrame, or you could also interpret each column in a *pandas* DataFrame as a Series instead, where its index is going to be the same index used for the rows in the DataFrame. It all depends on your needs for each situation.\n",
        "\n",
        "DataFrames are usually the most used structures in *pandas*, since in most uses of the library people deal with lots of data, usually arranged into tables, which are easily represented inside a DataFrame."
      ],
      "id": "80f1017e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51bcc1e0"
      },
      "source": [
        "#### DataFrame creation\n",
        "There are several possible ways of creating a *pandas* DataFrame. Let's take a look into the most used and common methods:\n",
        "\n",
        "From a CSV file - If you have a comma-separated values file (csv) in which the values are present in a tabular fashion, you can import it directly into your DataFrame by using the [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv) method. The only required parameter to this method is the path and name to the csv file you wish to read from. This way, the first row in the CSV file will by default name the columns, and from the second row onwards, the data will be imported as the values in the DataFrame. "
      ],
      "id": "51bcc1e0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05698389"
      },
      "source": [
        "url = 'https://github.com/daitan-innovation/daitan-ml-course-resources/blob/main/sample.csv?raw=true'\n",
        "df = pd.read_csv(url)\n",
        "df"
      ],
      "id": "05698389",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "294253ff"
      },
      "source": [
        "You can also write back to a CSV file using the `df.to_csv('file_name')` method:"
      ],
      "id": "294253ff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "527ef1cb"
      },
      "source": [
        "df.to_csv('new.csv')"
      ],
      "id": "527ef1cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "963e0432"
      },
      "source": [
        "Doing it in the way described above will also export the index data into the CSV, making it harder to read it later if you do not intend to use the index values in any meaningful way. To avoid that, pass also the parameter \"index=False\" so the index does not get written into the CSV file:"
      ],
      "id": "963e0432"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d91d42a"
      },
      "source": [
        "local_df = pd.read_csv('new.csv')\n",
        "local_df"
      ],
      "id": "0d91d42a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "254a7047"
      },
      "source": [
        "df.to_csv('new.csv', index=False)"
      ],
      "id": "254a7047",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "320c4aa7"
      },
      "source": [
        "local_df = pd.read_csv('new.csv')\n",
        "local_df"
      ],
      "id": "320c4aa7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ed9d891"
      },
      "source": [
        "From a list - As DataFrames can be considered a two-dimensional data structure, we can also use a one or two-dimensional list to create them. NumPy arrays can also be used in this same fashion."
      ],
      "id": "6ed9d891"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "764694da"
      },
      "source": [
        "df = pd.DataFrame([[1,'A'],[2,'Z'],[3,'B'],[4,'X']])\n",
        "df"
      ],
      "id": "764694da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae778cd5"
      },
      "source": [
        "Notice that using a one-dimensional list will generate a DataFrame with only one column:"
      ],
      "id": "ae778cd5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bf6c922"
      },
      "source": [
        "df = pd.DataFrame([5,4,3,2,1])\n",
        "df"
      ],
      "id": "8bf6c922",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb632d1b"
      },
      "source": [
        "To create a DataFrame with a single row, use only one list inside another list:"
      ],
      "id": "bb632d1b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "076bc0c6"
      },
      "source": [
        "df = pd.DataFrame([[2,4,6,8]])\n",
        "df"
      ],
      "id": "076bc0c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96eea03e"
      },
      "source": [
        "#### DataFrame metadata\n",
        "\n",
        "Let's now take a look into some information related to a DataFrame that is not the values contained in its rows."
      ],
      "id": "96eea03e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2753c50"
      },
      "source": [
        "[Column](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html) names:\n",
        "\n",
        "The names of the columns names or labels are useful to identify each of the columns in your DataFrame. We can check that in the examples above, some DataFrames were created without specifying the column names, which then default to zero-based iteger increments (0, 1, 2, ...). In order to properly name the columns when creating a DataFrame using the DataFrame() method, you can pass the \"columns\" parameter with a list of names for the columns:"
      ],
      "id": "a2753c50"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f2af8b3"
      },
      "source": [
        "df = pd.DataFrame([[2,4,6,8]], columns=['first', 'second', 'third', 'fourth'])\n",
        "df"
      ],
      "id": "3f2af8b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "612221b3"
      },
      "source": [
        "Column [data types](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html):\n",
        "\n",
        "Each column in a single DataFrame is evaluated to be from a single data type (dtype). If there is several different data types within a same column, *pandas* will identify that column's dtype as the most generic type that could contemplate all of the values in that column. To check what types are present in your df, you can use `df.dtypes`:"
      ],
      "id": "612221b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42954907"
      },
      "source": [
        "df = pd.read_csv('new.csv')\n",
        "df"
      ],
      "id": "42954907",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "103d2516"
      },
      "source": [
        "df.dtypes"
      ],
      "id": "103d2516",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1df01fa0"
      },
      "source": [
        "Above we can check that the values from `sample_column_b` were classified as \"int64\", which is the default choice for integer values, but in case we know we could use only a 32-bit integer for the whole column, we could force the DataFrame to use that instead. In order to \"force\" a dtype interpretation when reading a DataFrame from a CSV file, we can use the `dtype` parameter and pass a dictionary like that:"
      ],
      "id": "1df01fa0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "851a35b5"
      },
      "source": [
        "df = pd.read_csv('new.csv', dtype={'sample_column_b': np.int32})\n",
        "df"
      ],
      "id": "851a35b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea00b280"
      },
      "source": [
        "df.dtypes"
      ],
      "id": "ea00b280",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c518242a"
      },
      "source": [
        "DataFrame [shape](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html):\n",
        "\n",
        "The shape of a DataFrame simply tells its dimensions; i.e. how many rows and columns it is currently made of (excluding the index). The return is a tuple that says: `(number_of_rows, number_of_columns)`"
      ],
      "id": "c518242a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a12acdb"
      },
      "source": [
        "df.shape"
      ],
      "id": "9a12acdb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45d31ffd"
      },
      "source": [
        "DataFrame [index](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html):\n",
        "\n",
        "Similarly to a Series, DataFrames also contain indexes. In the examples we saw so far, they appear as the left-most column, which does not have a name on top, and is by default a zero-based incremental integer (0, 1, 2, ...). In the example of the DataFrame read from the CSV file seen above, we can see it as the **bold 0, 1, 2, 3, 4** numbers on the left-hand side, with one number for each row. Indexes in a DataFrame serve the purpose of identifying the rows. If wanted, just like in a Series, an index can be specified to be something different from the default one. Let's see an example where we use `sample_column_a` as the index for the DataFrame read from the CSV:"
      ],
      "id": "45d31ffd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e65168a"
      },
      "source": [
        "custom_index_df = pd.read_csv('new.csv', index_col='sample_column_a')\n",
        "custom_index_df"
      ],
      "id": "6e65168a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5c258c"
      },
      "source": [
        "Now we can see that, as we have already specified a custom index based on another column (using the parameter `index_col='column_name'`) the default index has not been generated, and instead we are using the values of that specified column as the index for this new DataFrame (which also show up in **bold**). We can also notice that the name of the column `sample_column_a` appears on top of the column, instead of having an unnamed index, like in previous examples, and is slightly below the column names that are not indexes."
      ],
      "id": "fe5c258c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "153ee285"
      },
      "source": [
        "#### DataFrame data selection and access\n",
        "\n",
        "If desired, we can select just fewer columns of the whole DataFrame by using this notation: `df[['wanted', 'columns']]`:"
      ],
      "id": "153ee285"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8bbd23c"
      },
      "source": [
        "df = pd.read_csv('new.csv')\n",
        "df"
      ],
      "id": "e8bbd23c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991daa52"
      },
      "source": [
        "df[['sample_column_b']]"
      ],
      "id": "991daa52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26c3d21a"
      },
      "source": [
        "In order to check the first and last rows in a DataFrame, we can use the [head(n)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) and [tail(n)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html) methods, where n is the number of rows to be checked from the top or from the bottom (If no number is passed, the default is going to be 5):"
      ],
      "id": "26c3d21a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e53901e"
      },
      "source": [
        "df.head(3)"
      ],
      "id": "3e53901e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82211bfa"
      },
      "source": [
        "df.tail(3)"
      ],
      "id": "82211bfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f986ab8a"
      },
      "source": [
        "To select a row based on its **index**, we can use the [`df.loc[index]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) notation, which will yield all rows that contain that same **index**.\n",
        "- If only one row satisfies the index identification, it will be returned as a Series, where its values's index is the names of each column in the original DataFrame.\n",
        "- If more than one row satisfy the index, a sub-DataFrame will be returned, with all the rows that apply.\n",
        "- If no rows satisfy the index, then a KeyError will be thrown."
      ],
      "id": "f986ab8a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00842827"
      },
      "source": [
        "df.loc[1]"
      ],
      "id": "00842827",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "453a2d0c"
      },
      "source": [
        "custom_index_df = pd.read_csv('new.csv', index_col='sample_column_a')\n",
        "custom_index_df"
      ],
      "id": "453a2d0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8371208"
      },
      "source": [
        "custom_index_df.loc['A']"
      ],
      "id": "b8371208",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "689c2bb3"
      },
      "source": [
        "We can also access the exact value in a row (identified by its index) and column (identified by its name) using this notation:\n",
        "<br/>`df.loc['index_value', 'column_name']`\n",
        "<br/>The result will be the single value found."
      ],
      "id": "689c2bb3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a715afac"
      },
      "source": [
        "df.loc[1, 'sample_column_a']"
      ],
      "id": "a715afac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddfa4b86"
      },
      "source": [
        "To select a row purely based on which position it is located in the DataFrame, respecting its current order, we can use the [`df.iloc[position]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) notation (similarly to what we did previously in the Series section. As before, this is also zero-based.\n",
        "- The result for a single position specified will be a Series, with the index being the names of the DataFrame's columns.\n",
        "- The result for multiple positions specified will be a DataFrame containing the appropriate rows.\n",
        "- If any position cannot be found in the DataFrame, an out-of-bounds IndexError will be thrown."
      ],
      "id": "ddfa4b86"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ba5f84"
      },
      "source": [
        "df.iloc[4]"
      ],
      "id": "22ba5f84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df9db7ff"
      },
      "source": [
        "df.iloc[[0, 2]]"
      ],
      "id": "df9db7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6c545a5"
      },
      "source": [
        "In order to select data from your DataFrame based on the values of the columns, we can use this notation: `df[df['column_name'] == 'value']`. Notice that the `==` sign can be swapped for any other valid comparison evaluator. The result in this case will always be a sub-DataFrame even if no valid rows were found.\n",
        "\n",
        "Some examples:"
      ],
      "id": "f6c545a5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35dc5ced"
      },
      "source": [
        "df[df['sample_column_a'] == 'A']"
      ],
      "id": "35dc5ced",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95f15128"
      },
      "source": [
        "df[df['sample_column_b'] == 3]"
      ],
      "id": "95f15128",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c966eabe"
      },
      "source": [
        "df[df['sample_column_b'] == 0]"
      ],
      "id": "c966eabe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab2753fd"
      },
      "source": [
        "Let's now take a look at different ways in which we can iterate through the data in our DataFrames.\n",
        "\n",
        "One of the most common and straightforward ways to iterate through a DataFrame is to iterate through its rows. We can use a method called [`iterrows()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html) to achieve that. In each iteration, we receive a tuple with 2 values:\n",
        "- The first value (in the position 0) is the index of that row in the original DataFrame.\n",
        "- The second value (in the position 1) is a Series where the indexes are the column names and the values are the values themselves for that row and columns. The Series's name is going to be the same as the respective row's index.\n",
        "\n",
        "Let's take a look into a single iteration over the rows in the `df` DataFrame:"
      ],
      "id": "ab2753fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fdbb533"
      },
      "source": [
        "for item in df.iterrows():\n",
        "    print('The whole item:', item, '\\n')\n",
        "    print('Only the index:', item[0], '\\n')\n",
        "    print('Only the data:', item[1], '\\n')\n",
        "    break"
      ],
      "id": "0fdbb533",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5fc1647"
      },
      "source": [
        "Another useful way of iterating through a DataFrame is to iterate through its columns. We can use a method called [`iteritems()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iteritems.html) to do that. In each iteration, we also receive a tuple with 2 values:\n",
        "- The first value (in the position 0) is the name of the respective column in the original DataFrame.\n",
        "- The second value (in the position 1) is a Series where the indexes are the same indexes as in the original DataFrame, and its values are the respective values from that column and indexes. The Series's name is going to be the same as the respective column's name.\n",
        "\n",
        "Let's take a look into a single iteration over the rows in the `df` DataFrame:"
      ],
      "id": "d5fc1647"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44e93e8d"
      },
      "source": [
        "for item in df.iteritems():\n",
        "    print('The whole item:', item, '\\n')\n",
        "    print('Only the column name:', item[0], '\\n')\n",
        "    print('Only the data:', item[1], '\\n')\n",
        "    break"
      ],
      "id": "44e93e8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49a14253"
      },
      "source": [
        "#### Useful DataFrame methods\n",
        "\n",
        "Let's take a look now into some methods we can use to summarize and manipulate data within a single DataFrame.\n",
        "\n",
        "Statistical summarization of numerical data:\n",
        "\n",
        "Firstly, the [describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method provides a quick statistical summary of all numeric data present in your DataFrame, separated by column. The result is a new dataframe in which each column represents a column in your DataFrame that contains a numeric dtype. Each row then accounts for a different statistic measure. Take a look below:"
      ],
      "id": "49a14253"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c36175"
      },
      "source": [
        "df.describe()"
      ],
      "id": "61c36175",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93d0397c"
      },
      "source": [
        "Removing duplicates:\n",
        "\n",
        "If you need to remove duplicated rows from your DataFrame, you can use the [`drop_duplicates()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) method. By default, it is going to drop only the rows that are completely identical (i.e. the values from every column is the same, excluding the index). Let's see some examples:"
      ],
      "id": "93d0397c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5add785"
      },
      "source": [
        "df.drop_duplicates()"
      ],
      "id": "d5add785",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18c3070c"
      },
      "source": [
        "In the example above, no rows were dropped because all rows are different if we consider the values from all columns. Let's now consider only the `sample_column_a` by using the parameter `subset=['columns_to', 'be_considered']` so that rows 0 and 4 can be considered duplicates and remove one of them:"
      ],
      "id": "18c3070c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2157bf"
      },
      "source": [
        "df.drop_duplicates(subset=['sample_column_a'])"
      ],
      "id": "0f2157bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dd33bdc"
      },
      "source": [
        "We can also check that by default only the first row was kept. This can also be tweaked by providing the `keep='first/last/False'` parameter, with the following results:\n",
        "- **keep=first**: keeps only the first duplicate (it is the default keep value).\n",
        "- **keep=last**: keeps only the last duplicate.\n",
        "- **keep=False**: removes all duplicates\n",
        "\n",
        "It is also important to notice that this method will by default return a new DataFrame with the changes applied. In order to actually remove the duplicates from the original DataFrame, use the `inplace=True` parameter."
      ],
      "id": "6dd33bdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d547be02"
      },
      "source": [
        "Filling nulls:\n",
        "\n",
        "Just like in a Series, we can also directly fill all null values in a DataFrame by using the [`fillna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) method. Onca again, just like in the `drop_duplicates()` method described above, a new DataFrame will be returned. If you with to apply this change to the same DataFrame, pass `inplace=True` as a parameter."
      ],
      "id": "d547be02"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d403f05"
      },
      "source": [
        "test_nan_df = pd.DataFrame([[0, np.nan], [np.nan, 2]])\n",
        "test_nan_df"
      ],
      "id": "0d403f05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9262708d"
      },
      "source": [
        "test_nan_df.fillna(10)"
      ],
      "id": "9262708d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ce050d"
      },
      "source": [
        "Sorting data:\n",
        "\n",
        "Sorting data in a DataFrame can be useful in a lot of different situations, especially because the default display methods of Jupyter, as well as the `head()` and `tail()` only show the first or last few rows of your DataFrame. There are 2 main methods for sorting your DataFrame:\n",
        "- [`sort_index()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_index.html)\n",
        "- [`sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html)\n",
        "\n",
        "By using `sort_index()`, you do not specify which column is used in the sort (as it is already specified to be the index). You can however use some parameters like `ascending=False` if you want to sort from the highest index to the lowest (by default the opposite will occur), as well as `axis=1` if you want to sort the columns instead of the rows.\n",
        "\n",
        "Let's check some examples:"
      ],
      "id": "b6ce050d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2acc51c0"
      },
      "source": [
        "df.sort_index(ascending=False)"
      ],
      "id": "2acc51c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97988161"
      },
      "source": [
        "df.sort_index(axis=1, ascending=False)"
      ],
      "id": "97988161",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bfbb033"
      },
      "source": [
        "By using `sort_values()`, you need to specify the parameter `by=['columns_to', 'be_considered']`. The order in which you specify this array matters as it will be used in case several values from the left-most column specified collide. Let's take a look into some examples:"
      ],
      "id": "3bfbb033"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29a6ecbb"
      },
      "source": [
        "df.sort_values(by=['sample_column_b'], ascending=False)"
      ],
      "id": "29a6ecbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b090b5c"
      },
      "source": [
        "You can also specify a list of sorting orders in the `ascending=` parameter. It should signal what is the sorting order for each of the columns specified in the `by=` parameter:"
      ],
      "id": "4b090b5c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a934d39"
      },
      "source": [
        "df.sort_values(by=['sample_column_a', 'sample_column_b'], ascending=[False, True])"
      ],
      "id": "2a934d39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f7a443"
      },
      "source": [
        "Finally, notice that just like in the `drop_columns()` method, the sorting methods will also return a new DataFrame. If you wish to apply the sorting to the same DataFrame that you are using, pass the parameter `inplace=True`."
      ],
      "id": "30f7a443"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4a0d463"
      },
      "source": [
        "Grouping data:\n",
        "\n",
        "Sometimes you will need to aggregate or group data into buckets of similar features. *pandas* DataFrames provide you with the [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) method, which works very similarly to `GROUP BY` statements in SQL, for example. You will need to specify which are the columns that should be used in order to identify the different groups by passing the `by=['columns_to', 'be_considered']` parameter. The pure result of the `groupby()` method is going to be a collection of groups, which can each be interepreted as a tuple:\n",
        "- The first item in the tuple (position 0) is the identifier of the group, which can be a single value if you specified only one column in the `by=['single_column']` parameter, or a tuple if you specified multiple columns in the `by=['multiple', 'columns']` parameter.\n",
        "- The second and last item in the tuple (position 1) is a sub-DataFrame which contains all the rows that are part of that group.\n",
        "\n",
        "Let's take a quick look into the first group generated by doing a `df.groupby()` and grouping by the column `sample_column_a`."
      ],
      "id": "e4a0d463"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f9741da"
      },
      "source": [
        "for item in df.groupby(by=['sample_column_a']):\n",
        "    sub_df = item[1]\n",
        "    break\n",
        "sub_df"
      ],
      "id": "5f9741da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6db49a48"
      },
      "source": [
        "Usually, you will want to chain some aggregation function into the result of the groupby, so that it gets applied to all groups at the same time, and the result is a new DataFrame, which will contain one value per group found.\n",
        "\n",
        "Let's see an example of that:"
      ],
      "id": "6db49a48"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "192de2eb"
      },
      "source": [
        "df"
      ],
      "id": "192de2eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ccb62f2"
      },
      "source": [
        "df.groupby(by=['sample_column_a']).min()"
      ],
      "id": "7ccb62f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ced757b"
      },
      "source": [
        "df.groupby(by=['sample_column_a']).max()"
      ],
      "id": "4ced757b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da56a8b3"
      },
      "source": [
        "Applying functions to the whole DataFrame:\n",
        "\n",
        "Similarly to what we saw before in the Series section, it is also possible to apply a function to the whole DataFrame, by using the [`apply()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method. Just like before, we can either define a function explicitly and pass it as the parameter for `apply()`, or quickly define a function inside the signature itself, using lambda notation.\n",
        "However, in the case of the DataFrame version of this method, there is an important catch. We can pass the parameter `axis=0` (which already is the default value) or `axis=1`. Here's what it means:\n",
        "- `axis=0`: Means that the function will be applied to all columns, each execution of `apply()` will receive one of the columns as its argument (in the format of a Series). This is the default value of `axis` if not otherwise specified.\n",
        "- `axis=1`: Means that the function will be applied to all rows, each execution of `apply()` will receive one of the rows as its argument (in the format of a Series).\n",
        "\n",
        "Let's check some examples for both:"
      ],
      "id": "da56a8b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b811120a"
      },
      "source": [
        "df"
      ],
      "id": "b811120a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e65cc3dc"
      },
      "source": [
        "df.apply(lambda column: column.min())"
      ],
      "id": "e65cc3dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54c79f6a"
      },
      "source": [
        "In the example above, what happened was that the method `min()` was used in each of the 2 Series (one for each column, as `axis` was not specified and thus was 0). This resulted in us getting back the minimum of the `sample_column_a`, which was \"A\", and the minimum of the `sample_column_b`, which was 1. Notice that the resulting Series had the column names as its index, and the resulting value of the operation as its values."
      ],
      "id": "54c79f6a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db74cd82"
      },
      "source": [
        "df.apply(lambda row: f\"{row.loc['sample_column_a']}->{row.loc['sample_column_b']*2}\", axis=1)"
      ],
      "id": "db74cd82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c02a55"
      },
      "source": [
        "In the example above, we created a custom string for each of the original rows (`axis=1`), by using some selecting operators into each Series, and then multiplied the numeric value, found in `sample_column_b` by 2. Notice that the resulting Series has the original DataFrame's index as its own index and the result of the applied lambda function as its values."
      ],
      "id": "86c02a55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7433b5bd"
      },
      "source": [
        "Is it possible, then, to apply a single function to all elements of the DataFrame?\n",
        "\n",
        "Yes. In order to do that, we can use the [`applymap()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.applymap.html) method. It will apply the same function to every single element in the DataFrame, instead of applying to each row or column. Let's take a look into an example:"
      ],
      "id": "7433b5bd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07bb2f58"
      },
      "source": [
        "df.applymap(lambda element: len(str(element)))"
      ],
      "id": "07bb2f58",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a76b4380"
      },
      "source": [
        "In the example above, we just applied a simple verification of how long the string of the element is, which ended up keeping the original shape of the DataFrame, as well as the indexes and column names."
      ],
      "id": "a76b4380"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbcc22a5"
      },
      "source": [
        "#### Operations between 2 DataFrames\n",
        "\n",
        "Let's now take a look into several different methods and operations that use 2 DataFrames.\n",
        "\n",
        "Merge and join:\n",
        "\n",
        "The methods [`merge()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) and [`join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html) are very similar to each other in what they accomplish, but differ in the flexibility of how to do it. Both can be very similar to a common database join, where we get rows from two different tables and extend the columns based on some join key. However, here is the difference between these two methods in *pandas*:\n",
        "- With `merge()`, you can choose any column or set of columns that should match from both tables in order to perform the operation.\n",
        "- With `join()`, the index is automatically used as the key for the operation.\n",
        "\n",
        "Let's take a look into some examples:"
      ],
      "id": "dbcc22a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed86c96e"
      },
      "source": [
        "Merge example:"
      ],
      "id": "ed86c96e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b92aca3"
      },
      "source": [
        "df"
      ],
      "id": "7b92aca3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "429938c2"
      },
      "source": [
        "df_to_merge = pd.DataFrame([['A', 'aa'], ['C', 'cc'], ['D', 'dd']], columns=['sample_column_a', 'extra_column'])\n",
        "df_to_merge"
      ],
      "id": "429938c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f7a0f70"
      },
      "source": [
        "df.merge(df_to_merge, on=['sample_column_a'])"
      ],
      "id": "8f7a0f70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "719a8631"
      },
      "source": [
        "In the example above we can notice that the `merge()` method used the following notation: \n",
        "<br/>`left_df.merge(right_df, on=['merge', 'columns'])`\n",
        "<br/>There are also some other common parameters that we could add in order to make this merge more like what we need in different cases:\n",
        "- `how='left'/'right'/'outer'/'inner'/'cross'`: This chooses what the [merge method](https://www.w3schools.com/sql/sql_join.asp) should be. Default is `inner`.\n",
        "- `right_on=['right_df', 'merge_columns']`: If the column names of the columns that should be merged do not match across both DataFrames, you can instead use this field to signal which columns from the right DataFrame should be used in the merge.\n",
        "- `left_on=['left_table', 'merge_columns']`: Same as the one described above, but signales the used columns in the left DataFrame."
      ],
      "id": "719a8631"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a58ae83"
      },
      "source": [
        "Join example:"
      ],
      "id": "8a58ae83"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c49c4581"
      },
      "source": [
        "df"
      ],
      "id": "c49c4581",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a85a3032"
      },
      "source": [
        "df2 = pd.DataFrame([300, 400, 500], columns=['new_column'])\n",
        "df2"
      ],
      "id": "a85a3032",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eddc952"
      },
      "source": [
        "df.join(df2)"
      ],
      "id": "4eddc952",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb64d755"
      },
      "source": [
        "In the example above we can notice that the `join()` method used the following notation: \n",
        "<br/>`left_df.join(right_df)`\n",
        "<br/>As this method is less flexible than the one described before, there are less custom parameters that can be passed and in this course we will only focus only on `how`:\n",
        "- `how='left'/'right'/'outer'/'inner'`: This chooses what the [merge method](https://www.w3schools.com/sql/sql_join.asp) should be. Differently from the merge method, the default here is `left`."
      ],
      "id": "cb64d755"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8706dc69"
      },
      "source": [
        "Append:\n",
        "\n",
        "The [`append()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.append.html#pandas.DataFrame.append) method simply appends the rows from the specified DataFrame into the calling DataFrame. Columns that are not present in either DataFrame will be added as well, adding null values for the rows from the dataset that do not contain the column. Passing the parameter `ignore_index=True` will regenerate the index using zero-based incremental integers; otherwise the indices will be kepto from both DataFrames."
      ],
      "id": "8706dc69"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d1abc1e"
      },
      "source": [
        "df"
      ],
      "id": "8d1abc1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7d2c97b"
      },
      "source": [
        "df2 = pd.DataFrame([[300, 'z'], [400, 'x'],  [500, 'y']], columns=['sample_column_a', 'new_column'])\n",
        "df2"
      ],
      "id": "f7d2c97b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8547f8bc"
      },
      "source": [
        "df.append(df2, ignore_index=True)"
      ],
      "id": "8547f8bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738943e0"
      },
      "source": [
        "### Thank you!\n",
        "\n",
        "This concludes the basics on *pandas*. We hope that this helps as an introduction to the subject and that the next steps into data science with Python become easier for you to grasp with this knowledge. If you want to further your learning into this subject, please check the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/index.html), a [quick introduction guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) and some [common uses of the tool](https://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html#cookbook).\n",
        "\n",
        "We wish you all a happy learning and good luck!"
      ],
      "id": "738943e0"
    }
  ]
}